{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09b5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh_feature_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0d4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.ts_input_dim=100\n",
    "CFG.ts_hidden_dim=128\n",
    "CFG.epoches=300\n",
    "CFG.jump_out_value=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca428c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_features_PCA=np.load(\"./numpy/extracted_features_PCA.npy\")\n",
    "# test_feature_PCA=np.load(\"./numpy/test_feature_PCA.npy\")\n",
    "# train_extracted_pca_large=np.load(\"./numpy/train_extracted_pca_large.npy\")\n",
    "# test_extracted_pca_large=np.load(\"./numpy/test_extracted_pca_large.npy\")\n",
    "train_extracted_minmax_large=np.load(\"./numpy/train_extracted_minmax_train_0322.npy\")\n",
    "test_extracted_minmax_large=np.load(\"./numpy/test_extracted_minmax_train_0322.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c20925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.kernel=CFG.kernel\n",
    "        super().__init__()\n",
    "\n",
    "    def get_kernel(self,train_data,test_data):\n",
    "        \"\"\"\n",
    "\n",
    "        :param train_data: 类的一个实例，字典取[\"input\"], batch_size(1024)*input_feature，\n",
    "        :param test_data:  1*input_feature\n",
    "        :return: a diag matrix representing weights\n",
    "        \"\"\"\n",
    "        #n_batch=len(train_data[\"input\"],dtye=torch.float)\n",
    "        n_batch=CFG.batch_size\n",
    "        diag=torch.zeros(n_batch,dtype=float).to(CFG.device)\n",
    "        for idx in range(n_batch):\n",
    "            diag[idx]=torch.exp(\n",
    "                -F.pairwise_distance(test_data[0],train_data[idx],p=2)\n",
    "            )\n",
    "        #diag[torch.where(diag < torch.median(diag))] = 0\n",
    "        diag[torch.where(diag < torch.quantile(diag, 0.75,dim=0 ,\n",
    "                                               keepdim=False,\n",
    "                                               interpolation='nearest'))]=0\n",
    "        return torch.diag(diag/torch.sum(diag))\n",
    "\n",
    "    def get_kernel_loss(self, pred_batch, target_batch, train_data, test_data):\n",
    "        \"\"\"\n",
    "        :param pred: batch_size*1,\n",
    "        :param target: batch_size*1, 实例[\"RUL\"]\n",
    "        :param train_data:\n",
    "        :param test_data:\n",
    "        :return: the loss\n",
    "        \"\"\"\n",
    "        n_batch = CFG.batch_size\n",
    "        loss = torch.zeros(1, dtype=torch.float).to(CFG.device)\n",
    "        kernel=self.get_kernel(train_data,test_data).float()\n",
    "        if self.kernel:\n",
    "            residual = (target_batch - pred_batch).float()\n",
    "            loss = torch.inner(torch.mm(kernel,residual).squeeze(-1),residual.squeeze(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            residual = (target_batch - pred_batch).squeeze(-1).float()\n",
    "            loss = torch.inner(residual, residual)/n_batch\n",
    "            return loss\n",
    "\n",
    "    def forward(self,pred_batch, target_batch, train_data, test_data):\n",
    "        return self.get_kernel_loss(pred_batch, target_batch, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd629c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=CFG.device\n",
    "train_dataset_expend_encoded = AircraftDataset_expend_feature_extraction(df_train, train_label, torch.FloatTensor(train_extracted_minmax_large),True)\n",
    "train_encoded_loader = DataLoader(\n",
    "    train_dataset_expend_encoded,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_dataset = AircraftDataset_no_expend_feature_extraction(df_train, test_label, torch.FloatTensor(test_extracted_minmax_large))\n",
    "test_encoded_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False)\n",
    "# classifier_in = KMeans(n_clusters=CFG.num_in_feature_classes, random_state=CFG.seed).fit(get_input(test_dataset))\n",
    "model = CustomModel2(CFG.ts_input_dim,CFG.ts_hidden_dim).to(device)\n",
    "loss_function = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "924d37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_data in train_encoded_loader:None\n",
    "for test_data in test_encoded_loader:None\n",
    "model=CustomModel2(100,128).to(device)\n",
    "pred_batch = model(train_data[\"input\"].to(device))\n",
    "target_batch = train_data[\"RUL\"].unsqueeze(-1).to(device)\n",
    "data_train= train_data[\"input\"].to(device)\n",
    "data_test = test_data[\"input\"].to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc92203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a98bdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[ 0.0410,  0.2118,  0.0752,  ..., -0.3104,  0.5431,  0.6916],\n",
       "         [ 0.5427,  0.5204,  0.5497,  ...,  0.9737, -1.0000, -0.9952],\n",
       "         [ 0.7125,  0.7751,  0.7384,  ..., -0.9302,  0.9664,  0.9736],\n",
       "         ...,\n",
       "         [-0.0590,  0.0774,  0.1193,  ...,  0.1563, -0.6369, -0.6247],\n",
       "         [ 0.0976,  0.1580,  0.2107,  ...,  0.9102, -1.0000, -0.9988],\n",
       "         [ 0.0902,  0.0717,  0.1259,  ..., -0.1833,  0.9249,  0.9392]]),\n",
       " 'RUL': tensor([254,  25,   9,  ..., 290,  82,  90]),\n",
       " 'mode': tensor([[1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         ...,\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_encoded_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92cd3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_his_unit_in_unit_0(train_loader, test_loader_batch, in_unit_index, folder, RESUME=False):\n",
    "    \"\"\"\n",
    "    传入 in_unit 的feature来获得kernel完成预测\n",
    "    :param train_loader: 含有多个batch, 是字典形式的\n",
    "    :param test_loader_batch: test loader 的一个batch， 是字典形式的\n",
    "    :param in_unit_index: 记录in unit的index\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # last_loss = torch.tensor([1e5], dtype=torch.float).to(CFG.device)\n",
    "    optimizer = getattr(torch.optim, CFG.optimizer)(model.parameters(), lr=CFG.lr)  # 优化器\n",
    "    scheduler = getattr(torch.optim.lr_scheduler, CFG.scheduler)(optimizer, gamma=CFG.sc_Gamma)  # 指数型学习率\n",
    "    start_epoch = -1\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    if RESUME:\n",
    "        path_checkpoint = folder + '/ckpt_unit_%s.pth' % (str(in_unit_index))  # 断点路径\n",
    "        if os.path.isfile(path_checkpoint):\n",
    "            checkpoint = torch.load(path_checkpoint, map_location=CFG.device)  # 加载断点\n",
    "\n",
    "            model.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\n",
    "\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数\n",
    "            start_epoch = checkpoint['epoch']  # 设置开始的epoch\n",
    "            scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        else:\n",
    "            None\n",
    "\n",
    "    for epoch in range(start_epoch + 1, CFG.epoches):\n",
    "        for data in train_loader:\n",
    "            pred_batch = model(data[\"input\"].to(device))\n",
    "            target_batch = data[\"RUL\"].unsqueeze(-1).to(device)\n",
    "            train_data = data[\"input\"].to(device)\n",
    "            test_data = test_loader_batch[\"input\"].to(device)\n",
    "            loss = loss_function(pred_batch, target_batch, train_data, test_data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        # scheduler.step(loss,last_loss)\n",
    "        # last_loss = loss\n",
    "        if CFG.print_training_process and epoch % 100 == 0:\n",
    "            print(f\"epoch:{epoch}, loss:{loss.item()},lr:{optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "            checkpoint = {\n",
    "                \"net\": model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            if not os.path.isdir(folder):\n",
    "                os.mkdir(folder)\n",
    "            torch.save(checkpoint, folder + '/ckpt_unit_%s.pth' % (str(in_unit_index)))\n",
    "\n",
    "        if loss.mean() < CFG.jump_out_value:\n",
    "            save_model_weights(model, f\"model_in_unit_{in_unit_index}.pt\",\n",
    "                               cp_folder=folder)\n",
    "            break\n",
    "        elif epoch == (CFG.epoches - 1):\n",
    "            print(f\"epoch:{epoch}, loss:{loss.item()}\")\n",
    "            save_model_weights(model, f\"model_in_unit_{in_unit_index}.pt\",\n",
    "                               cp_folder=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31f71f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:1774.0853271484375,lr:0.09890548353295386\n",
      "\n",
      " -> Saving weights to ./model_checkpoints_ts/kernel without classify\\model_in_unit_0.pt\n",
      "\n",
      "epoch:0, loss:1733.062255859375,lr:0.09890548353295386\n",
      "\n",
      " -> Saving weights to ./model_checkpoints_ts/kernel without classify\\model_in_unit_1.pt\n",
      "\n",
      "epoch:0, loss:716.5570678710938,lr:0.09890548353295386\n",
      "\n",
      " -> Saving weights to ./model_checkpoints_ts/kernel without classify\\model_in_unit_2.pt\n",
      "\n",
      "epoch:0, loss:2904.9970703125,lr:0.09890548353295386\n",
      "\n",
      " -> Saving weights to ./model_checkpoints_ts/kernel without classify\\model_in_unit_3.pt\n",
      "\n",
      "epoch:0, loss:3670.66259765625,lr:0.09890548353295386\n",
      "\n",
      " -> Saving weights to ./model_checkpoints_ts/kernel without classify\\model_in_unit_4.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, test_data in enumerate(test_encoded_loader):\n",
    "    fit_his_unit_in_unit_0(train_encoded_loader, test_data, idx,\n",
    "                                   \"./model_checkpoints_ts/kernel without classify\", RESUME=False)\n",
    "    if idx==4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef5887a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_his_unit(train_loader,path, RESUME=False):\n",
    "    start_epoch=-1\n",
    "    optimizer = getattr(torch.optim, CFG.optimizer)(model.parameters(), lr=CFG.lr)  # 优化器\n",
    "    scheduler = getattr(torch.optim.lr_scheduler, CFG.scheduler)(optimizer, gamma=CFG.sc_Gamma)  # 指数型学习率\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "    if RESUME:\n",
    "        path_checkpoint = path+'/ckpt.pth'   # 断点路径\n",
    "        checkpoint = torch.load(path_checkpoint)  # 加载断点\n",
    "\n",
    "        model.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\n",
    "\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数\n",
    "        start_epoch = checkpoint['epoch']  # 设置开始的epoch\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch+1,CFG.epoches):\n",
    "        for data in train_loader:\n",
    "            pred_batch = model(data[\"input\"].to(device))\n",
    "            target_batch = data[\"RUL\"].unsqueeze(-1).to(device)\n",
    "            loss = F.mse_loss(pred_batch, target_batch.float())\n",
    "        # loss = loss_function(pred_batch, target_batch, train_data, test_data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        # scheduler.step(loss,last_loss)\n",
    "        # last_loss = loss\n",
    "        if CFG.print_training_process and epoch % 100 == 0:\n",
    "            print(f\"epoch:{epoch}, loss:{loss.item()},lr:{optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "            checkpoint = {\n",
    "                \"net\": model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            if not os.path.isdir(path):\n",
    "                os.mkdir(path)\n",
    "            torch.save(checkpoint,\n",
    "                       path+'/ckpt.pth')\n",
    "        if loss.mean() < CFG.jump_out_value:\n",
    "            save_model_weights(model, \"model_his.pt\",\n",
    "                               cp_folder=path)\n",
    "            break\n",
    "        elif epoch == (CFG.epoches - 1):\n",
    "            print(f\"epoch:{epoch}, loss:{loss.item()}\")\n",
    "            save_model_weights(model, \"model_his.pt\",\n",
    "                               cp_folder=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adce2ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:4590.6025390625,lr:0.09890548353295386\n",
      "epoch:100, loss:136.086181640625,lr:0.032904660865335375\n",
      "\n",
      " -> Saving weights to ./model_checkpoints_ts/no kernel no classify\\model_his.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_his_unit(train_encoded_loader,\"./model_checkpoints_ts/no kernel no classify\", RESUME=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1744ad8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model_checkpoints_ts/no kernel no classify/model_his.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efe43476",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1=[]\n",
    "for idx,test_data in enumerate(test_encoded_loader):\n",
    "    pred_1_q=model(test_data[\"input\"].to(device)).item()\n",
    "    pred_1.append(pred_1_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a4d0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUL\n",
    "path = DATA_PATH+'RUL.csv'\n",
    "RUL_frame = pd.read_csv(path, header=None)\n",
    "RUL = RUL_frame.values[:, 0]\n",
    "RUL_y=pd.Series(RUL)\n",
    "RUL_y.index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7dcd04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2360.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.ceil((abs(np.array(pred_1)-RUL))))#目测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "420bf238",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2=[]\n",
    "for idx,test_data in enumerate(test_encoded_loader):\n",
    "    path=\"./model_checkpoints_ts/kernel without classify/\"+f\"model_in_unit_{idx}.pt\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    pred_2_q=model(test_data[\"input\"].to(device)).item()\n",
    "    pred_2.append(pred_2_q)\n",
    "    if idx==4:break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63496614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.4951286315918,\n",
       " 48.358062744140625,\n",
       " 31.401683807373047,\n",
       " 151.714111328125,\n",
       " 74.61571502685547]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44f96ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44,  51,  27, 120, 101,  99,  71,  55,  55,  66,  77, 115, 115,\n",
       "        31, 108,  56, 136, 132,  85,  56,  18, 119,  78,   9,  58,  11,\n",
       "        88, 144, 124,  89,  79,  55,  71,  65,  87, 137, 145,  22,   8,\n",
       "        41, 131, 115, 128,  69, 111,   7, 137,  55, 135,  11,  78, 120,\n",
       "        87,  87,  55,  93,  88,  40,  49, 128, 129,  58, 117,  28, 115,\n",
       "        87,  92, 103, 100,  63,  35,  45,  99, 117,  45,  27,  86,  20,\n",
       "        18, 133,  15,   6, 145, 104,  56,  25,  68, 144,  41,  51,  81,\n",
       "        14,  67,  10, 127, 113, 123,  17,   8,  28], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0324feca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.ceil((abs(np.array(pred_2)-RUL[0:5]))))*20#目测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd94c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
